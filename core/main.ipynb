{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils import data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from MnistNet import MnistNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "NUM_EPOCHS = 100  # original paper\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "LR_DECAY = 0.0005\n",
    "LR_INIT = 0.01\n",
    "IMAGE_DIM = 28  # pixels\n",
    "NUM_CLASSES = 10  # 10 classes for mnist dataset\n",
    "DEVICE_IDS = [0, 1, 2, 3]  # GPUs to use\n",
    "\n",
    "# define pytorch device - useful for device-agnostic execution\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# print the seed value\n",
    "# seed = torch.initial_seed()\n",
    "# print('Used seed : {}'.format(seed))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): MnistNet(\n",
      "    (net): Sequential(\n",
      "      (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=1536, out_features=4096, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MnistNet created\n"
     ]
    }
   ],
   "source": [
    "mnistnet = MnistNet(num_classes=NUM_CLASSES).to(device)\n",
    "# train on multiple GPUs\n",
    "mnistnet = torch.nn.parallel.DataParallel(mnistnet, device_ids=DEVICE_IDS)\n",
    "print(mnistnet)\n",
    "print('MnistNet created')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megame/opt/anaconda3/envs/mnist_nn/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "y = y.astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X = X.reshape(X.shape[0], 1, 28, 28)\n",
    "print(X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(),\n",
    "                              torch.from_numpy(y_train).long())\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=8,\n",
    "                          drop_last=True,\n",
    "                          batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float(),\n",
    "                             torch.from_numpy(y_test).long())\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=8,\n",
    "                          drop_last=True,\n",
    "                          batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "# the one that WORKS\n",
    "optimizer = optim.Adam(params=mnistnet.parameters(), lr=0.0001)\n",
    "### BELOW is the setting proposed by the original paper - which doesn't train....\n",
    "# optimizer = optim.SGD(\n",
    "#     params=alexnet.parameters(),\n",
    "#     lr=LR_INIT,\n",
    "#     momentum=MOMENTUM,\n",
    "#     weight_decay=LR_DECAY)\n",
    "print('Optimizer created')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Scheduler created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megame/opt/anaconda3/envs/mnist_nn/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tStep: 10 \tLoss: 2.3370 \tAcc: 0.296875\n",
      "Epoch: 1 \tStep: 20 \tLoss: 1.3040 \tAcc: 0.4765625\n",
      "Epoch: 1 \tStep: 30 \tLoss: 0.7916 \tAcc: 0.796875\n",
      "Epoch: 1 \tStep: 40 \tLoss: 0.7768 \tAcc: 0.7890625\n",
      "Epoch: 1 \tStep: 50 \tLoss: 0.5921 \tAcc: 0.7734375\n",
      "Epoch: 1 \tStep: 60 \tLoss: 0.4054 \tAcc: 0.875\n",
      "Epoch: 1 \tStep: 70 \tLoss: 0.4375 \tAcc: 0.8515625\n",
      "Epoch: 1 \tStep: 80 \tLoss: 0.2400 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 90 \tLoss: 0.3686 \tAcc: 0.875\n",
      "Epoch: 1 \tStep: 100 \tLoss: 0.3923 \tAcc: 0.90625\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: 0.009873581118881702\n",
      "\tmodule.net.0.weight - param_avg: -0.0011626502964645624\n",
      "\tmodule.net.0.bias - grad_avg: 0.0002546436444390565\n",
      "\tmodule.net.0.bias - param_avg: -0.001768997055478394\n",
      "\tmodule.classifier.1.weight - grad_avg: 1.9491248167469166e-05\n",
      "\tmodule.classifier.1.weight - param_avg: -0.00015869141498114914\n",
      "\tmodule.classifier.1.bias - grad_avg: 5.179708296054741e-06\n",
      "\tmodule.classifier.1.bias - param_avg: -0.0003106079821009189\n",
      "\tmodule.classifier.4.weight - grad_avg: 8.842838724376634e-06\n",
      "\tmodule.classifier.4.weight - param_avg: -6.0749494878109545e-05\n",
      "\tmodule.classifier.4.bias - grad_avg: 5.380672519095242e-06\n",
      "\tmodule.classifier.4.bias - param_avg: -1.4463927072938532e-05\n",
      "\tmodule.classifier.6.weight - grad_avg: -3.7252903539730653e-10\n",
      "\tmodule.classifier.6.weight - param_avg: 0.00010023963113781065\n",
      "\tmodule.classifier.6.bias - grad_avg: 4.656612873077393e-10\n",
      "\tmodule.classifier.6.bias - param_avg: -0.0012093132827430964\n",
      "Epoch: 1 \tStep: 110 \tLoss: 0.2415 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 120 \tLoss: 0.3599 \tAcc: 0.90625\n",
      "Epoch: 1 \tStep: 130 \tLoss: 0.2940 \tAcc: 0.8984375\n",
      "Epoch: 1 \tStep: 140 \tLoss: 0.1932 \tAcc: 0.9453125\n",
      "Epoch: 1 \tStep: 150 \tLoss: 0.1778 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 160 \tLoss: 0.2172 \tAcc: 0.9140625\n",
      "Epoch: 1 \tStep: 170 \tLoss: 0.2110 \tAcc: 0.9453125\n",
      "Epoch: 1 \tStep: 180 \tLoss: 0.3546 \tAcc: 0.921875\n",
      "Epoch: 1 \tStep: 190 \tLoss: 0.2471 \tAcc: 0.90625\n",
      "Epoch: 1 \tStep: 200 \tLoss: 0.2617 \tAcc: 0.9140625\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: 0.0034004764165729284\n",
      "\tmodule.net.0.weight - param_avg: -0.0015154697466641665\n",
      "\tmodule.net.0.bias - grad_avg: 0.00011359863128745928\n",
      "\tmodule.net.0.bias - param_avg: -0.002766671823337674\n",
      "\tmodule.classifier.1.weight - grad_avg: 1.852441710070707e-05\n",
      "\tmodule.classifier.1.weight - param_avg: -0.00015788960445206612\n",
      "\tmodule.classifier.1.bias - grad_avg: 5.570459961745655e-06\n",
      "\tmodule.classifier.1.bias - param_avg: -0.00028090752311982214\n",
      "\tmodule.classifier.4.weight - grad_avg: 9.117607987718657e-06\n",
      "\tmodule.classifier.4.weight - param_avg: -5.396442793426104e-05\n",
      "\tmodule.classifier.4.bias - grad_avg: 6.473476787505206e-06\n",
      "\tmodule.classifier.4.bias - param_avg: 3.5911361919716e-06\n",
      "\tmodule.classifier.6.weight - grad_avg: 1.396983917434369e-10\n",
      "\tmodule.classifier.6.weight - param_avg: 9.970196697395295e-05\n",
      "\tmodule.classifier.6.bias - grad_avg: 7.450580707946131e-10\n",
      "\tmodule.classifier.6.bias - param_avg: -0.0012024820316582918\n",
      "Epoch: 1 \tStep: 210 \tLoss: 0.4568 \tAcc: 0.8671875\n",
      "Epoch: 1 \tStep: 220 \tLoss: 0.1577 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 230 \tLoss: 0.1902 \tAcc: 0.9296875\n",
      "Epoch: 1 \tStep: 240 \tLoss: 0.0917 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 250 \tLoss: 0.1516 \tAcc: 0.96875\n",
      "Epoch: 1 \tStep: 260 \tLoss: 0.1784 \tAcc: 0.9453125\n",
      "Epoch: 1 \tStep: 270 \tLoss: 0.1657 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 280 \tLoss: 0.2057 \tAcc: 0.9453125\n",
      "Epoch: 1 \tStep: 290 \tLoss: 0.1417 \tAcc: 0.953125\n",
      "Epoch: 1 \tStep: 300 \tLoss: 0.2359 \tAcc: 0.9140625\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: 0.0019765382166951895\n",
      "\tmodule.net.0.weight - param_avg: -0.0016958200139924884\n",
      "\tmodule.net.0.bias - grad_avg: 0.00011721175542334095\n",
      "\tmodule.net.0.bias - param_avg: -0.0035876547917723656\n",
      "\tmodule.classifier.1.weight - grad_avg: 4.211078703519888e-06\n",
      "\tmodule.classifier.1.weight - param_avg: -0.00018296770576853305\n",
      "\tmodule.classifier.1.bias - grad_avg: 1.0526348432904342e-06\n",
      "\tmodule.classifier.1.bias - param_avg: -0.00030441879061982036\n",
      "\tmodule.classifier.4.weight - grad_avg: -1.7653954955676454e-06\n",
      "\tmodule.classifier.4.weight - param_avg: -5.7136254326906055e-05\n",
      "\tmodule.classifier.4.bias - grad_avg: -2.444066012685653e-06\n",
      "\tmodule.classifier.4.bias - param_avg: 4.713983798865229e-07\n",
      "\tmodule.classifier.6.weight - grad_avg: -1.8626451769865326e-10\n",
      "\tmodule.classifier.6.weight - param_avg: 9.698441863292828e-05\n",
      "\tmodule.classifier.6.bias - grad_avg: 8.381902949494702e-10\n",
      "\tmodule.classifier.6.bias - param_avg: -0.00120006303768605\n",
      "Epoch: 1 \tStep: 310 \tLoss: 0.2399 \tAcc: 0.9453125\n",
      "Epoch: 1 \tStep: 320 \tLoss: 0.0794 \tAcc: 0.9765625\n",
      "Epoch: 1 \tStep: 330 \tLoss: 0.1327 \tAcc: 0.96875\n",
      "Epoch: 1 \tStep: 340 \tLoss: 0.3022 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 350 \tLoss: 0.2171 \tAcc: 0.9296875\n",
      "Epoch: 1 \tStep: 360 \tLoss: 0.1448 \tAcc: 0.96875\n",
      "Epoch: 1 \tStep: 370 \tLoss: 0.1236 \tAcc: 0.953125\n",
      "Epoch: 1 \tStep: 380 \tLoss: 0.1394 \tAcc: 0.9765625\n",
      "Epoch: 1 \tStep: 390 \tLoss: 0.0966 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 400 \tLoss: 0.1739 \tAcc: 0.9375\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: -0.001443223562091589\n",
      "\tmodule.net.0.weight - param_avg: -0.001867478247731924\n",
      "\tmodule.net.0.bias - grad_avg: 7.987651042640209e-05\n",
      "\tmodule.net.0.bias - param_avg: -0.004440814256668091\n",
      "\tmodule.classifier.1.weight - grad_avg: 5.675341071764706e-06\n",
      "\tmodule.classifier.1.weight - param_avg: -0.00020712120749522\n",
      "\tmodule.classifier.1.bias - grad_avg: 2.388082521065371e-06\n",
      "\tmodule.classifier.1.bias - param_avg: -0.0003365625161677599\n",
      "\tmodule.classifier.4.weight - grad_avg: 6.923045475559775e-06\n",
      "\tmodule.classifier.4.weight - param_avg: -6.418074190150946e-05\n",
      "\tmodule.classifier.4.bias - grad_avg: 5.933313786954386e-06\n",
      "\tmodule.classifier.4.bias - param_avg: -1.3728305930271745e-05\n",
      "\tmodule.classifier.6.weight - grad_avg: -1.3038515822572094e-09\n",
      "\tmodule.classifier.6.weight - param_avg: 9.819519618758932e-05\n",
      "\tmodule.classifier.6.bias - grad_avg: -1.3038515822572094e-09\n",
      "\tmodule.classifier.6.bias - param_avg: -0.001190324081107974\n",
      "Epoch: 1 \tStep: 410 \tLoss: 0.0768 \tAcc: 0.9765625\n",
      "Epoch: 1 \tStep: 420 \tLoss: 0.0970 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 430 \tLoss: 0.0767 \tAcc: 0.9765625\n",
      "Epoch: 1 \tStep: 440 \tLoss: 0.0957 \tAcc: 0.9765625\n",
      "Epoch: 1 \tStep: 450 \tLoss: 0.1770 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 460 \tLoss: 0.0604 \tAcc: 0.9921875\n",
      "Epoch: 2 \tStep: 470 \tLoss: 0.1461 \tAcc: 0.953125\n",
      "Epoch: 2 \tStep: 480 \tLoss: 0.0769 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 490 \tLoss: 0.1132 \tAcc: 0.9453125\n",
      "Epoch: 2 \tStep: 500 \tLoss: 0.1225 \tAcc: 0.9609375\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: 0.0015519242733716965\n",
      "\tmodule.net.0.weight - param_avg: -0.0019494998268783092\n",
      "\tmodule.net.0.bias - grad_avg: 3.341986666782759e-05\n",
      "\tmodule.net.0.bias - param_avg: -0.005075013730674982\n",
      "\tmodule.classifier.1.weight - grad_avg: 1.9049207367061172e-07\n",
      "\tmodule.classifier.1.weight - param_avg: -0.0002288852265337482\n",
      "\tmodule.classifier.1.bias - grad_avg: 4.0921918298408855e-08\n",
      "\tmodule.classifier.1.bias - param_avg: -0.00034401181619614363\n",
      "\tmodule.classifier.4.weight - grad_avg: 1.5075613646331476e-06\n",
      "\tmodule.classifier.4.weight - param_avg: -7.111198647180572e-05\n",
      "\tmodule.classifier.4.bias - grad_avg: 9.7962777090288e-07\n",
      "\tmodule.classifier.4.bias - param_avg: -2.1633615688188e-05\n",
      "\tmodule.classifier.6.weight - grad_avg: 8.381902949494702e-10\n",
      "\tmodule.classifier.6.weight - param_avg: 9.763851994648576e-05\n",
      "\tmodule.classifier.6.bias - grad_avg: 1.0244548542814869e-09\n",
      "\tmodule.classifier.6.bias - param_avg: -0.0011817559134215117\n",
      "Epoch: 2 \tStep: 510 \tLoss: 0.0756 \tAcc: 0.9765625\n",
      "Epoch: 2 \tStep: 520 \tLoss: 0.1634 \tAcc: 0.9296875\n",
      "Epoch: 2 \tStep: 530 \tLoss: 0.2343 \tAcc: 0.9296875\n",
      "Epoch: 2 \tStep: 540 \tLoss: 0.2249 \tAcc: 0.9375\n",
      "Epoch: 2 \tStep: 550 \tLoss: 0.0994 \tAcc: 0.953125\n",
      "Epoch: 2 \tStep: 560 \tLoss: 0.0703 \tAcc: 0.984375\n",
      "Epoch: 2 \tStep: 570 \tLoss: 0.1259 \tAcc: 0.953125\n",
      "Epoch: 2 \tStep: 580 \tLoss: 0.0922 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 590 \tLoss: 0.0569 \tAcc: 0.9765625\n",
      "Epoch: 2 \tStep: 600 \tLoss: 0.0421 \tAcc: 0.9921875\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: -0.005326291546225548\n",
      "\tmodule.net.0.weight - param_avg: -0.0020472423639148474\n",
      "\tmodule.net.0.bias - grad_avg: -3.436604674789123e-05\n",
      "\tmodule.net.0.bias - param_avg: -0.005713090766221285\n",
      "\tmodule.classifier.1.weight - grad_avg: -1.0439250672789058e-06\n",
      "\tmodule.classifier.1.weight - param_avg: -0.00025056570302695036\n",
      "\tmodule.classifier.1.bias - grad_avg: -6.915292942721862e-07\n",
      "\tmodule.classifier.1.bias - param_avg: -0.0003636522451415658\n",
      "\tmodule.classifier.4.weight - grad_avg: -2.471373136359034e-06\n",
      "\tmodule.classifier.4.weight - param_avg: -7.719273708062246e-05\n",
      "\tmodule.classifier.4.bias - grad_avg: -2.663043233042117e-06\n",
      "\tmodule.classifier.4.bias - param_avg: -2.7811352993012406e-05\n",
      "\tmodule.classifier.6.weight - grad_avg: -9.313225884932663e-11\n",
      "\tmodule.classifier.6.weight - param_avg: 9.7117037512362e-05\n",
      "\tmodule.classifier.6.bias - grad_avg: -3.7252903539730653e-10\n",
      "\tmodule.classifier.6.bias - param_avg: -0.0011856728233397007\n",
      "Epoch: 2 \tStep: 610 \tLoss: 0.0960 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 620 \tLoss: 0.0821 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 630 \tLoss: 0.0687 \tAcc: 0.9765625\n",
      "Epoch: 2 \tStep: 640 \tLoss: 0.0693 \tAcc: 0.984375\n",
      "Epoch: 2 \tStep: 650 \tLoss: 0.0496 \tAcc: 0.984375\n",
      "Epoch: 2 \tStep: 660 \tLoss: 0.0708 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 670 \tLoss: 0.0964 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 680 \tLoss: 0.1289 \tAcc: 0.9453125\n",
      "Epoch: 2 \tStep: 690 \tLoss: 0.0229 \tAcc: 0.9921875\n",
      "Epoch: 2 \tStep: 700 \tLoss: 0.0876 \tAcc: 0.9765625\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: -0.0003451733791735023\n",
      "\tmodule.net.0.weight - param_avg: -0.002154669724404812\n",
      "\tmodule.net.0.bias - grad_avg: 2.1561243556789123e-05\n",
      "\tmodule.net.0.bias - param_avg: -0.006272152066230774\n",
      "\tmodule.classifier.1.weight - grad_avg: 3.388217237443314e-06\n",
      "\tmodule.classifier.1.weight - param_avg: -0.00026679792790673673\n",
      "\tmodule.classifier.1.bias - grad_avg: 3.28390513004706e-07\n",
      "\tmodule.classifier.1.bias - param_avg: -0.0003668952267616987\n",
      "\tmodule.classifier.4.weight - grad_avg: -2.5331385131721618e-06\n",
      "\tmodule.classifier.4.weight - param_avg: -8.181434532161802e-05\n",
      "\tmodule.classifier.4.bias - grad_avg: -3.333868562549469e-06\n",
      "\tmodule.classifier.4.bias - param_avg: -3.262890822952613e-05\n",
      "\tmodule.classifier.6.weight - grad_avg: 7.450580707946131e-10\n",
      "\tmodule.classifier.6.weight - param_avg: 9.551087714498863e-05\n",
      "\tmodule.classifier.6.bias - grad_avg: 1.280568540096283e-09\n",
      "\tmodule.classifier.6.bias - param_avg: -0.001179891056381166\n",
      "Epoch: 2 \tStep: 710 \tLoss: 0.2088 \tAcc: 0.9296875\n",
      "Epoch: 2 \tStep: 720 \tLoss: 0.1181 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 730 \tLoss: 0.1156 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 740 \tLoss: 0.0756 \tAcc: 0.9765625\n",
      "Epoch: 2 \tStep: 750 \tLoss: 0.0580 \tAcc: 0.984375\n",
      "Epoch: 2 \tStep: 760 \tLoss: 0.1113 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 770 \tLoss: 0.0646 \tAcc: 0.9765625\n",
      "Epoch: 2 \tStep: 780 \tLoss: 0.0850 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 790 \tLoss: 0.0854 \tAcc: 0.9765625\n",
      "Epoch: 2 \tStep: 800 \tLoss: 0.1103 \tAcc: 0.96875\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: 0.0014204850886017084\n",
      "\tmodule.net.0.weight - param_avg: -0.0022240830585360527\n",
      "\tmodule.net.0.bias - grad_avg: 0.00011802920926129445\n",
      "\tmodule.net.0.bias - param_avg: -0.006805470213294029\n",
      "\tmodule.classifier.1.weight - grad_avg: 2.7797525490314e-06\n",
      "\tmodule.classifier.1.weight - param_avg: -0.0002915029472205788\n",
      "\tmodule.classifier.1.bias - grad_avg: 2.960919118777383e-07\n",
      "\tmodule.classifier.1.bias - param_avg: -0.00039648188976570964\n",
      "\tmodule.classifier.4.weight - grad_avg: 9.205984383697796e-07\n",
      "\tmodule.classifier.4.weight - param_avg: -9.190282435156405e-05\n",
      "\tmodule.classifier.4.bias - grad_avg: 4.3418708628450986e-07\n",
      "\tmodule.classifier.4.bias - param_avg: -5.082231655251235e-05\n",
      "\tmodule.classifier.6.weight - grad_avg: -2.793967834868738e-10\n",
      "\tmodule.classifier.6.weight - param_avg: 9.723666880745441e-05\n",
      "\tmodule.classifier.6.bias - grad_avg: 9.313225884932663e-11\n",
      "\tmodule.classifier.6.bias - param_avg: -0.0011702504707500339\n",
      "Epoch: 2 \tStep: 810 \tLoss: 0.0814 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 820 \tLoss: 0.0842 \tAcc: 0.9765625\n",
      "Epoch: 2 \tStep: 830 \tLoss: 0.1172 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 840 \tLoss: 0.1401 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 850 \tLoss: 0.0632 \tAcc: 0.984375\n",
      "Epoch: 2 \tStep: 860 \tLoss: 0.1285 \tAcc: 0.9375\n",
      "Epoch: 2 \tStep: 870 \tLoss: 0.0795 \tAcc: 0.9765625\n",
      "Epoch: 2 \tStep: 880 \tLoss: 0.1475 \tAcc: 0.96875\n"
     ]
    }
   ],
   "source": [
    "# multiply LR by 1 / 10 after every 30 epochs\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "print('LR Scheduler created')\n",
    "\n",
    "# start training!!\n",
    "print('Starting training...')\n",
    "total_steps = 1\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    lr_scheduler.step()\n",
    "    for imgs, classes in train_loader:\n",
    "        imgs, classes = imgs.to(device), classes.to(device)\n",
    "\n",
    "        # calculate the loss\n",
    "        output = mnistnet(imgs)\n",
    "        loss = F.cross_entropy(output, classes)\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log the information and add to tensorboard\n",
    "        if total_steps % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                _, preds = torch.max(output, 1)\n",
    "                accuracy = torch.sum(preds == classes)/len(classes)\n",
    "\n",
    "                print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n",
    "                      .format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n",
    "\n",
    "        # print out gradient values and parameter average values\n",
    "        if total_steps % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                # print and save the grad of the parameters\n",
    "                # also print and save parameter values\n",
    "                print('*' * 10)\n",
    "                for name, parameter in mnistnet.named_parameters():\n",
    "                    if parameter.grad is not None:\n",
    "                        avg_grad = torch.mean(parameter.grad)\n",
    "                        print('\\t{} - grad_avg: {}'.format(name, avg_grad))\n",
    "                        # tbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_steps)\n",
    "                        # tbwriter.add_histogram('grad/{}'.format(name),\n",
    "                        #                       parameter.grad.cpu().numpy(), total_steps)\n",
    "                    if parameter.data is not None:\n",
    "                        avg_weight = torch.mean(parameter.data)\n",
    "                        print('\\t{} - param_avg: {}'.format(name, avg_weight))\n",
    "                        # tbwriter.add_histogram('weight/{}'.format(name),\n",
    "                        #                       parameter.data.cpu().numpy(), total_steps)\n",
    "                        # tbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_steps)\n",
    "\n",
    "        total_steps += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mnistnet.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_out = 0\n",
    "        total_out = 0\n",
    "        for pics, lbls in test_loader:\n",
    "            out = mnistnet(pics)\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            total_out += lbls.shape[0]\n",
    "            correct_out += (pred == lbls).sum().item()\n",
    "    loss_current = criterion(out, lbls)\n",
    "    print(correct_out / total_out, loss_current)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}