{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import fetch_openml\n",
    "from torch.utils import data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from MnistNet import MnistNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "NUM_EPOCHS = 20  # original paper\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "LR_DECAY = 0.0005\n",
    "LR_INIT = 0.01\n",
    "IMAGE_DIM = 28  # pixels\n",
    "NUM_CLASSES = 10  # 10 classes for mnist dataset\n",
    "DEVICE_IDS = [0, 1, 2, 3]  # GPUs to use\n",
    "\n",
    "# define pytorch device - useful for device-agnostic execution\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# print the seed value\n",
    "# seed = torch.initial_seed()\n",
    "# print('Used seed : {}'.format(seed))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): MnistNet(\n",
      "    (net): Sequential(\n",
      "      (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (5): ReLU()\n",
      "      (6): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
      "      (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU()\n",
      "      (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU()\n",
      "      (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=256, out_features=4096, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MnistNet created\n"
     ]
    }
   ],
   "source": [
    "mnistnet = MnistNet(num_classes=NUM_CLASSES).to(device)\n",
    "# train on multiple GPUs\n",
    "mnistnet = torch.nn.parallel.DataParallel(mnistnet, device_ids=DEVICE_IDS)\n",
    "print(mnistnet)\n",
    "print('MnistNet created')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megame/opt/anaconda3/envs/mnist_nn/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "y = y.astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X = X.reshape(X.shape[0], 1, 28, 28)\n",
    "print(X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(),\n",
    "                              torch.from_numpy(y_train).long())\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=8,\n",
    "                          drop_last=True,\n",
    "                          batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float(),\n",
    "                             torch.from_numpy(y_test).long())\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=8,\n",
    "                          drop_last=True,\n",
    "                          batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer created\n"
     ]
    }
   ],
   "source": [
    "# create optimizer\n",
    "# the one that WORKS\n",
    "optimizer = optim.Adam(params=mnistnet.parameters(), lr=0.0001)\n",
    "### BELOW is the setting proposed by the original paper - which doesn't train....\n",
    "# optimizer = optim.SGD(\n",
    "#     params=alexnet.parameters(),\n",
    "#     lr=LR_INIT,\n",
    "#     momentum=MOMENTUM,\n",
    "#     weight_decay=LR_DECAY)\n",
    "print('Optimizer created')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Scheduler created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/megame/opt/anaconda3/envs/mnist_nn/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tStep: 10 \tLoss: 2.3169 \tAcc: 0.109375\n",
      "Epoch: 1 \tStep: 20 \tLoss: 2.2901 \tAcc: 0.21875\n",
      "Epoch: 1 \tStep: 30 \tLoss: 2.1668 \tAcc: 0.2109375\n",
      "Epoch: 1 \tStep: 40 \tLoss: 1.8075 \tAcc: 0.328125\n",
      "Epoch: 1 \tStep: 50 \tLoss: 1.6119 \tAcc: 0.34375\n",
      "Epoch: 1 \tStep: 60 \tLoss: 1.2769 \tAcc: 0.5078125\n",
      "Epoch: 1 \tStep: 70 \tLoss: 1.1567 \tAcc: 0.6015625\n",
      "Epoch: 1 \tStep: 80 \tLoss: 0.9103 \tAcc: 0.6640625\n",
      "Epoch: 1 \tStep: 90 \tLoss: 0.8601 \tAcc: 0.7109375\n",
      "Epoch: 1 \tStep: 100 \tLoss: 0.7625 \tAcc: 0.7421875\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: -0.009009919129312038\n",
      "\tmodule.net.0.weight - param_avg: 0.0007182385306805372\n",
      "\tmodule.net.0.bias - grad_avg: -0.00015122632612474263\n",
      "\tmodule.net.0.bias - param_avg: 2.0025254343636334e-05\n",
      "\tmodule.net.4.weight - grad_avg: -7.995805935934186e-05\n",
      "\tmodule.net.4.weight - param_avg: -0.00022586983686778694\n",
      "\tmodule.net.4.bias - grad_avg: -9.16624594538007e-06\n",
      "\tmodule.net.4.bias - param_avg: 1.2750132555083837e-05\n",
      "\tmodule.net.8.weight - grad_avg: -0.0001844556099968031\n",
      "\tmodule.net.8.weight - param_avg: 2.7071460863226093e-05\n",
      "\tmodule.net.8.bias - grad_avg: -0.00017594097880646586\n",
      "\tmodule.net.8.bias - param_avg: 0.00013892074639443308\n",
      "\tmodule.net.10.weight - grad_avg: -0.00016765788313932717\n",
      "\tmodule.net.10.weight - param_avg: 8.117923243844416e-06\n",
      "\tmodule.net.10.bias - grad_avg: -0.0008379406644962728\n",
      "\tmodule.net.10.bias - param_avg: 0.00016521690122317523\n",
      "\tmodule.net.12.weight - grad_avg: -2.7194128051633015e-05\n",
      "\tmodule.net.12.weight - param_avg: -5.751481694460381e-06\n",
      "\tmodule.net.12.bias - grad_avg: -0.0004987422144040465\n",
      "\tmodule.net.12.bias - param_avg: -9.460623550694436e-05\n",
      "\tmodule.classifier.1.weight - grad_avg: 1.0433241186547093e-06\n",
      "\tmodule.classifier.1.weight - param_avg: -4.739087671623565e-05\n",
      "\tmodule.classifier.1.bias - grad_avg: -7.698987610638142e-05\n",
      "\tmodule.classifier.1.bias - param_avg: 0.00025670716422609985\n",
      "\tmodule.classifier.4.weight - grad_avg: -4.758220484291087e-07\n",
      "\tmodule.classifier.4.weight - param_avg: 0.00036189050297252834\n",
      "\tmodule.classifier.4.bias - grad_avg: -2.4603523343103006e-06\n",
      "\tmodule.classifier.4.bias - param_avg: 0.0007210270268842578\n",
      "\tmodule.classifier.6.weight - grad_avg: 0.0\n",
      "\tmodule.classifier.6.weight - param_avg: -0.0005366550758481026\n",
      "\tmodule.classifier.6.bias - grad_avg: 1.8626451769865326e-10\n",
      "\tmodule.classifier.6.bias - param_avg: 0.001190567621961236\n",
      "Epoch: 1 \tStep: 110 \tLoss: 0.6502 \tAcc: 0.8203125\n",
      "Epoch: 1 \tStep: 120 \tLoss: 0.3345 \tAcc: 0.8828125\n",
      "Epoch: 1 \tStep: 130 \tLoss: 0.3926 \tAcc: 0.8671875\n",
      "Epoch: 1 \tStep: 140 \tLoss: 0.4809 \tAcc: 0.859375\n",
      "Epoch: 1 \tStep: 150 \tLoss: 0.4462 \tAcc: 0.8515625\n",
      "Epoch: 1 \tStep: 160 \tLoss: 0.4129 \tAcc: 0.875\n",
      "Epoch: 1 \tStep: 170 \tLoss: 0.3527 \tAcc: 0.8984375\n",
      "Epoch: 1 \tStep: 180 \tLoss: 0.2330 \tAcc: 0.953125\n",
      "Epoch: 1 \tStep: 190 \tLoss: 0.3308 \tAcc: 0.8984375\n",
      "Epoch: 1 \tStep: 200 \tLoss: 0.2153 \tAcc: 0.9296875\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: 0.001017846050672233\n",
      "\tmodule.net.0.weight - param_avg: 0.00029980725958012044\n",
      "\tmodule.net.0.bias - grad_avg: -2.466525802446995e-05\n",
      "\tmodule.net.0.bias - param_avg: -0.0007439491455443203\n",
      "\tmodule.net.4.weight - grad_avg: -0.00039064508746378124\n",
      "\tmodule.net.4.weight - param_avg: -0.00033901736605912447\n",
      "\tmodule.net.4.bias - grad_avg: -3.611417196225375e-05\n",
      "\tmodule.net.4.bias - param_avg: -9.906954073812813e-05\n",
      "\tmodule.net.8.weight - grad_avg: -4.2360174120403826e-05\n",
      "\tmodule.net.8.weight - param_avg: 2.4161616238416173e-05\n",
      "\tmodule.net.8.bias - grad_avg: -5.616837370325811e-05\n",
      "\tmodule.net.8.bias - param_avg: 0.00010927803668892011\n",
      "\tmodule.net.10.weight - grad_avg: -4.469665509532206e-05\n",
      "\tmodule.net.10.weight - param_avg: 8.879882443579845e-06\n",
      "\tmodule.net.10.bias - grad_avg: -0.000277253013337031\n",
      "\tmodule.net.10.bias - param_avg: 0.00019696290837600827\n",
      "\tmodule.net.12.weight - grad_avg: 9.32731018110644e-06\n",
      "\tmodule.net.12.weight - param_avg: -4.376449851406505e-06\n",
      "\tmodule.net.12.bias - grad_avg: 0.00017177833069581538\n",
      "\tmodule.net.12.bias - param_avg: -4.5430686441250145e-05\n",
      "\tmodule.classifier.1.weight - grad_avg: 1.2666319548770844e-07\n",
      "\tmodule.classifier.1.weight - param_avg: -6.0430240409914404e-05\n",
      "\tmodule.classifier.1.bias - grad_avg: -7.218455721158534e-05\n",
      "\tmodule.classifier.1.bias - param_avg: 0.0006520024035125971\n",
      "\tmodule.classifier.4.weight - grad_avg: -2.7125324777443893e-06\n",
      "\tmodule.classifier.4.weight - param_avg: 0.00040608993731439114\n",
      "\tmodule.classifier.4.bias - grad_avg: -2.5237039153580554e-05\n",
      "\tmodule.classifier.4.bias - param_avg: 0.0008158650016412139\n",
      "\tmodule.classifier.6.weight - grad_avg: 1.396983917434369e-10\n",
      "\tmodule.classifier.6.weight - param_avg: -0.0006247410783544183\n",
      "\tmodule.classifier.6.bias - grad_avg: 1.1175871339474952e-09\n",
      "\tmodule.classifier.6.bias - param_avg: 0.001180291292257607\n",
      "Epoch: 1 \tStep: 210 \tLoss: 0.2219 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 220 \tLoss: 0.2704 \tAcc: 0.9296875\n",
      "Epoch: 1 \tStep: 230 \tLoss: 0.2667 \tAcc: 0.9140625\n",
      "Epoch: 1 \tStep: 240 \tLoss: 0.1742 \tAcc: 0.9453125\n",
      "Epoch: 1 \tStep: 250 \tLoss: 0.2420 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 260 \tLoss: 0.1253 \tAcc: 0.96875\n",
      "Epoch: 1 \tStep: 270 \tLoss: 0.2646 \tAcc: 0.921875\n",
      "Epoch: 1 \tStep: 280 \tLoss: 0.2865 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 290 \tLoss: 0.1667 \tAcc: 0.921875\n",
      "Epoch: 1 \tStep: 300 \tLoss: 0.2307 \tAcc: 0.90625\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: -0.0017637215787544847\n",
      "\tmodule.net.0.weight - param_avg: -1.4313350220618304e-05\n",
      "\tmodule.net.0.bias - grad_avg: -4.005516893812455e-05\n",
      "\tmodule.net.0.bias - param_avg: -0.0013347376370802522\n",
      "\tmodule.net.4.weight - grad_avg: 0.00012210790009703487\n",
      "\tmodule.net.4.weight - param_avg: -0.0003982661000918597\n",
      "\tmodule.net.4.bias - grad_avg: 1.410365803167224e-05\n",
      "\tmodule.net.4.bias - param_avg: -0.00022488813556265086\n",
      "\tmodule.net.8.weight - grad_avg: -3.4112897992599756e-05\n",
      "\tmodule.net.8.weight - param_avg: 1.798724042600952e-05\n",
      "\tmodule.net.8.bias - grad_avg: -5.995869287289679e-05\n",
      "\tmodule.net.8.bias - param_avg: 2.9187198379077017e-05\n",
      "\tmodule.net.10.weight - grad_avg: 1.3324040992301889e-05\n",
      "\tmodule.net.10.weight - param_avg: 8.980951861303765e-06\n",
      "\tmodule.net.10.bias - grad_avg: 3.2416541216662154e-05\n",
      "\tmodule.net.10.bias - param_avg: 0.00018491898663342\n",
      "\tmodule.net.12.weight - grad_avg: 1.4646354884462198e-06\n",
      "\tmodule.net.12.weight - param_avg: -5.958170731901191e-06\n",
      "\tmodule.net.12.bias - grad_avg: -0.00010061187640530989\n",
      "\tmodule.net.12.bias - param_avg: -4.9569578550290316e-05\n",
      "\tmodule.classifier.1.weight - grad_avg: -8.786803391558351e-07\n",
      "\tmodule.classifier.1.weight - param_avg: -6.000633948133327e-05\n",
      "\tmodule.classifier.1.bias - grad_avg: 1.1963382348767482e-05\n",
      "\tmodule.classifier.1.bias - param_avg: 0.0008781018550507724\n",
      "\tmodule.classifier.4.weight - grad_avg: 6.722906960021646e-07\n",
      "\tmodule.classifier.4.weight - param_avg: 0.0004374967538751662\n",
      "\tmodule.classifier.4.bias - grad_avg: 5.820745627715951e-06\n",
      "\tmodule.classifier.4.bias - param_avg: 0.0008946919697336853\n",
      "\tmodule.classifier.6.weight - grad_avg: -1.6298144778215118e-10\n",
      "\tmodule.classifier.6.weight - param_avg: -0.0006608261028304696\n",
      "\tmodule.classifier.6.bias - grad_avg: -7.450580707946131e-10\n",
      "\tmodule.classifier.6.bias - param_avg: 0.0011835284531116486\n",
      "Epoch: 1 \tStep: 310 \tLoss: 0.2540 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 320 \tLoss: 0.3841 \tAcc: 0.8828125\n",
      "Epoch: 1 \tStep: 330 \tLoss: 0.1461 \tAcc: 0.953125\n",
      "Epoch: 1 \tStep: 340 \tLoss: 0.1291 \tAcc: 0.9453125\n",
      "Epoch: 1 \tStep: 350 \tLoss: 0.1429 \tAcc: 0.953125\n",
      "Epoch: 1 \tStep: 360 \tLoss: 0.2213 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 370 \tLoss: 0.1381 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 380 \tLoss: 0.3723 \tAcc: 0.921875\n",
      "Epoch: 1 \tStep: 390 \tLoss: 0.1511 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 400 \tLoss: 0.3300 \tAcc: 0.9140625\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: 0.00015301175881177187\n",
      "\tmodule.net.0.weight - param_avg: -0.00025162275414913893\n",
      "\tmodule.net.0.bias - grad_avg: -3.8319853047141805e-05\n",
      "\tmodule.net.0.bias - param_avg: -0.001843322068452835\n",
      "\tmodule.net.4.weight - grad_avg: 0.0001537370408186689\n",
      "\tmodule.net.4.weight - param_avg: -0.00048248752136714756\n",
      "\tmodule.net.4.bias - grad_avg: 3.0678213079227135e-05\n",
      "\tmodule.net.4.bias - param_avg: -0.0003348723112139851\n",
      "\tmodule.net.8.weight - grad_avg: -6.383243089658208e-06\n",
      "\tmodule.net.8.weight - param_avg: 1.4817034752923064e-05\n",
      "\tmodule.net.8.bias - grad_avg: -1.2455578143999446e-05\n",
      "\tmodule.net.8.bias - param_avg: -4.805729986401275e-06\n",
      "\tmodule.net.10.weight - grad_avg: 1.0812094842549413e-05\n",
      "\tmodule.net.10.weight - param_avg: 8.749602784519084e-06\n",
      "\tmodule.net.10.bias - grad_avg: 9.66165607678704e-05\n",
      "\tmodule.net.10.bias - param_avg: 0.00019266239542048424\n",
      "\tmodule.net.12.weight - grad_avg: -2.020222382270731e-05\n",
      "\tmodule.net.12.weight - param_avg: -8.498631359543651e-06\n",
      "\tmodule.net.12.bias - grad_avg: -0.0005827583954669535\n",
      "\tmodule.net.12.bias - param_avg: -8.676497964188457e-05\n",
      "\tmodule.classifier.1.weight - grad_avg: -2.2837755295768147e-06\n",
      "\tmodule.classifier.1.weight - param_avg: -6.502782343886793e-05\n",
      "\tmodule.classifier.1.bias - grad_avg: 0.00019560045620892197\n",
      "\tmodule.classifier.1.bias - param_avg: 0.000995737500488758\n",
      "\tmodule.classifier.4.weight - grad_avg: 3.228855348424986e-06\n",
      "\tmodule.classifier.4.weight - param_avg: 0.0004515103646554053\n",
      "\tmodule.classifier.4.bias - grad_avg: 4.6350323827937245e-05\n",
      "\tmodule.classifier.4.bias - param_avg: 0.0009421229478903115\n",
      "\tmodule.classifier.6.weight - grad_avg: 2.561137135703717e-10\n",
      "\tmodule.classifier.6.weight - param_avg: -0.0007099235663190484\n",
      "\tmodule.classifier.6.bias - grad_avg: 1.0710209386033398e-09\n",
      "\tmodule.classifier.6.bias - param_avg: 0.0011801273794844747\n",
      "Epoch: 1 \tStep: 410 \tLoss: 0.1266 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 420 \tLoss: 0.2339 \tAcc: 0.9375\n",
      "Epoch: 1 \tStep: 430 \tLoss: 0.1611 \tAcc: 0.9609375\n",
      "Epoch: 1 \tStep: 440 \tLoss: 0.2038 \tAcc: 0.9296875\n",
      "Epoch: 1 \tStep: 450 \tLoss: 0.1370 \tAcc: 0.96875\n",
      "Epoch: 1 \tStep: 460 \tLoss: 0.1521 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 470 \tLoss: 0.1456 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 480 \tLoss: 0.0894 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 490 \tLoss: 0.2319 \tAcc: 0.9375\n",
      "Epoch: 2 \tStep: 500 \tLoss: 0.2632 \tAcc: 0.9140625\n",
      "**********\n",
      "\tmodule.net.0.weight - grad_avg: -0.0015035986434668303\n",
      "\tmodule.net.0.weight - param_avg: -0.0004740057629533112\n",
      "\tmodule.net.0.bias - grad_avg: -5.292880450724624e-05\n",
      "\tmodule.net.0.bias - param_avg: -0.00231732614338398\n",
      "\tmodule.net.4.weight - grad_avg: 7.709670171607286e-05\n",
      "\tmodule.net.4.weight - param_avg: -0.0005537623073905706\n",
      "\tmodule.net.4.bias - grad_avg: 1.7257836589124054e-05\n",
      "\tmodule.net.4.bias - param_avg: -0.000426633981987834\n",
      "\tmodule.net.8.weight - grad_avg: -3.206242763553746e-05\n",
      "\tmodule.net.8.weight - param_avg: 1.1874247320520226e-05\n",
      "\tmodule.net.8.bias - grad_avg: -6.863491580588743e-05\n",
      "\tmodule.net.8.bias - param_avg: -3.171591743011959e-05\n",
      "\tmodule.net.10.weight - grad_avg: -9.662298907642253e-06\n",
      "\tmodule.net.10.weight - param_avg: 8.598652129876427e-06\n",
      "\tmodule.net.10.bias - grad_avg: -0.00015132070984691381\n",
      "\tmodule.net.10.bias - param_avg: 0.00020024512195959687\n",
      "\tmodule.net.12.weight - grad_avg: -1.967149910342414e-05\n",
      "\tmodule.net.12.weight - param_avg: -7.5812886279891245e-06\n",
      "\tmodule.net.12.bias - grad_avg: -0.0006057763821445405\n",
      "\tmodule.net.12.bias - param_avg: -6.47105771349743e-05\n",
      "\tmodule.classifier.1.weight - grad_avg: 4.367858537079883e-07\n",
      "\tmodule.classifier.1.weight - param_avg: -6.506666250061244e-05\n",
      "\tmodule.classifier.1.bias - grad_avg: 0.00012206558312755078\n",
      "\tmodule.classifier.1.bias - param_avg: 0.0010730493813753128\n",
      "\tmodule.classifier.4.weight - grad_avg: 2.1895236841373844e-06\n",
      "\tmodule.classifier.4.weight - param_avg: 0.000457053683931008\n",
      "\tmodule.classifier.4.bias - grad_avg: 2.4418866814812645e-05\n",
      "\tmodule.classifier.4.bias - param_avg: 0.0009740973473526537\n",
      "\tmodule.classifier.6.weight - grad_avg: -9.313225884932663e-11\n",
      "\tmodule.classifier.6.weight - param_avg: -0.0007331272354349494\n",
      "\tmodule.classifier.6.bias - grad_avg: 1.396983917434369e-10\n",
      "\tmodule.classifier.6.bias - param_avg: 0.0011802326189354062\n",
      "Epoch: 2 \tStep: 510 \tLoss: 0.1311 \tAcc: 0.953125\n",
      "Epoch: 2 \tStep: 520 \tLoss: 0.0305 \tAcc: 1.0\n",
      "Epoch: 2 \tStep: 530 \tLoss: 0.1441 \tAcc: 0.9609375\n",
      "Epoch: 2 \tStep: 540 \tLoss: 0.1227 \tAcc: 0.96875\n",
      "Epoch: 2 \tStep: 550 \tLoss: 0.1919 \tAcc: 0.9453125\n"
     ]
    }
   ],
   "source": [
    "# multiply LR by 1 / 10 after every 30 epochs\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "print('LR Scheduler created')\n",
    "\n",
    "# start training!!\n",
    "print('Starting training...')\n",
    "total_steps = 1\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    lr_scheduler.step()\n",
    "    for imgs, classes in train_loader:\n",
    "        imgs, classes = imgs.to(device), classes.to(device)\n",
    "\n",
    "        # calculate the loss\n",
    "        output = mnistnet(imgs)\n",
    "        loss = F.cross_entropy(output, classes)\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log the information and add to tensorboard\n",
    "        if total_steps % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                _, preds = torch.max(output, 1)\n",
    "                accuracy = torch.sum(preds == classes)/len(preds)\n",
    "\n",
    "                print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n",
    "                      .format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n",
    "\n",
    "        # print out gradient values and parameter average values\n",
    "        if total_steps % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                # print and save the grad of the parameters\n",
    "                # also print and save parameter values\n",
    "                print('*' * 10)\n",
    "                for name, parameter in mnistnet.named_parameters():\n",
    "                    if parameter.grad is not None:\n",
    "                        avg_grad = torch.mean(parameter.grad)\n",
    "                        print('\\t{} - grad_avg: {}'.format(name, avg_grad))\n",
    "                        # tbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_steps)\n",
    "                        # tbwriter.add_histogram('grad/{}'.format(name),\n",
    "                        #                       parameter.grad.cpu().numpy(), total_steps)\n",
    "                    if parameter.data is not None:\n",
    "                        avg_weight = torch.mean(parameter.data)\n",
    "                        print('\\t{} - param_avg: {}'.format(name, avg_weight))\n",
    "                        # tbwriter.add_histogram('weight/{}'.format(name),\n",
    "                        #                       parameter.data.cpu().numpy(), total_steps)\n",
    "                        # tbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_steps)\n",
    "\n",
    "        total_steps += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mnistnet.eval()\n",
    "with torch.no_grad():\n",
    "    correct_out = 0\n",
    "    total_out = 0\n",
    "    for pics, lbls in test_loader:\n",
    "        out = mnistnet(pics)\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        total_out += lbls.shape[0]\n",
    "        correct_out += (pred == lbls).sum().item()\n",
    "\n",
    "loss_current = criterion(out, lbls)\n",
    "print(correct_out / total_out, loss_current)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "classes = [2, 3, 6, 8, 9, 1, 0, 4, 7, 5]\n",
    "for i in range(10):\n",
    "    img = cv2.imread(\"../my_dataset/\"+str(i+1)+\".png\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    sharp_filter = np.array([[-1, 0, -1],\n",
    "                             [0, 7, 0],\n",
    "                             [-1, 0, -1]])\n",
    "    img = cv2.filter2D(img, ddepth=-1, kernel=sharp_filter)\n",
    "\n",
    "    img = cv2.bitwise_not(img)\n",
    "    img = img.astype('float32')\n",
    "    data.append(img)\n",
    "\n",
    "data = np.reshape(data,(len(data), 1, 28, 28))\n",
    "# data = np.array(data)\n",
    "classes = np.array(classes)\n",
    "\n",
    "print(data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(data[i].reshape(28, 28), cmap='Greys')\n",
    "    plt.title(\"Цифра %d\" % classes[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_dataset = TensorDataset(torch.from_numpy(data).float(),\n",
    "                                   torch.from_numpy(classes).long())\n",
    "experiment_loader = DataLoader(experiment_dataset,\n",
    "                          shuffle=False,\n",
    "                          # pin_memory=True,\n",
    "                          # num_workers=8,\n",
    "                          # drop_last=True,\n",
    "\n",
    "                          batch_size=BATCH_SIZE)\n",
    "\n",
    "mnistnet.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in experiment_loader:\n",
    "        outputs = mnistnet(images)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(\"Test accuracy:\",\n",
    "        100 * correct / total, \"%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(data[i].reshape(28, 28), cmap='Greys')\n",
    "    plt.title(\"%d, pred: %d\" % (classes[i], predicted[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}